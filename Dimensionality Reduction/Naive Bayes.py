# -*- coding: utf-8 -*-
"""Mlbd_Assg_m22ma003Q1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GxpTXJJOl_0idVDbIT0f9vMzg88aGUPW
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.datasets as datasets
import torchvision.transforms as transforms
import time

# Define hyperparameters
num_epochs = 5
batch_size = 128
learning_rate = 0.001
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])
# Download MNIST dataset
train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
print(len(train_dataset))
test_dataset = datasets.MNIST(root='./data', train=False, transform=transform)
print(len(test_dataset))
# Define data loader
train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)

# Define model
class NaiveBayes(nn.Module):
    def __init__(self, num_features, num_classes):
        super(NaiveBayes, self).__init__()
        self.num_features = num_features
        self.num_classes = num_classes
        self.theta = nn.Parameter(torch.randn(num_classes, num_features))
        self.log_prior = nn.Parameter(torch.zeros(num_classes))
        
    def forward(self, x):
        log_probs = torch.matmul(x.view(-1, self.num_features), self.theta.t()) + self.log_prior
        return log_probs
    
    def update(self, x, y):
        class_counts = torch.zeros(self.num_classes)
        feature_counts = torch.zeros(self.num_classes, self.num_features)
        # print(x[0, :].view(1,-1).squeeze().shape)
        # print(x.shape, class_counts.shape, feature_counts.shape)
        for i in range(x.shape[0]):
            class_counts[y[i]] += 1
            feature_counts[y[i], :] += x[i, :].view(1, -1).squeeze()
        
        self.theta.data = feature_counts / class_counts.view(-1, 1)
        self.log_prior.data = torch.log(class_counts / class_counts.sum())
pred_data1=[]
# Initialize model and optimizer
model = NaiveBayes(num_features=28*28, num_classes=10)
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
start_time = time.time()
# Train model
for epoch in range(num_epochs):
    for i, (images, labels) in enumerate(train_loader):
        # Flatten images into a 784-dimensional vector
        images = images.view(-1, 28*28)
        
        # Compute log probabilities and loss
        log_probs = model(images)
        loss = nn.functional.nll_loss(log_probs, labels)
        
        # Zero gradients, perform backward pass, and update parameters
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        # Update model parameters with new data
        model.update(images, labels)
        
        # Print progress
        if (i+1) % 100 == 0:
            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()/len(train_loader)))
end_time = time.time()
training_time = end_time - start_time
# Test model
start_time = time.time()
with torch.no_grad():
    correct = 0
    total = 0
    
    for images, labels in test_loader:
        # Flatten images into a 784-dimensional vector
        images = images.view(-1, 28*28)
        
        # Compute log probabilities and predicted labels
        log_probs = model(images)
        _, predicted = torch.max(log_probs.data, 1)
        pred_data1.append(predicted)       
        # Update counts of correct and total predictions
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
        
    # Print accuracy
    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))
    end_time = time.time()
    testing_time = end_time - start_time
    print(f"Training and testing time are:{training_time:.4f}and {testing_time:.4f}")

print(f"Classification on the test set for n_dimension = 100 are:\n{pred_data1}")

"""Method 2 for implementation of naive bayes (streaming)"""

import numpy as np
# import psutil
import warnings
warnings.filterwarnings('ignore')
from matplotlib import pyplot as plt
from keras.datasets import mnist
# from datasketch import MinHash, MinHashLSH

import numpy as np
from keras.datasets import mnist
(X_train_data, y_train_data), (X_test_data, y_test_data) = mnist.load_data()
X_train = X_train_data.reshape(-1, 28*28)[:60000]
y_train = y_train_data[:60000]
X_test = X_test_data.reshape(-1, 28*28)[:10000]
y_test = y_test_data[:10000]

#converting the values in the binary
X_train=np.where(X_train>105,1,0)
X_test=np.where(X_test>105,1,0)
# x_test.ndim

import numpy as np
y_pred=[]
class StreamNBayes:
    def __init__(self, num_features, num_classes):
        self.num_features = num_features
        self.num_classes = num_classes
        self.counts = np.zeros((num_classes, num_features))
        self.class_counts = np.zeros(num_classes)
    
    def update(self, X, y):
        self.counts[y] += X
        self.class_counts[y] += 1
        
    def predict(self, X):
        probs = np.zeros(self.num_classes)
        for c in range(self.num_classes):
            feat_probs = (self.counts[c] + 1) / (self.class_counts[c] + 2)
            probs[c] = np.sum(X * np.log(feat_probs) + (1 - X) * np.log(1 - feat_probs))
        y_pred.append((np.argmax(probs)))
        return np.argmax(probs)

def naive_b(X_train, y_train, X_test, y_test, num_features):
    n_b = StreamNBayes(num_features, 10)
    start_time = time.time()
    for i in range(X_train.shape[0]):
        n_b.update(X_train[i], y_train[i])
    end_time = time.time()
    training_time = end_time - start_time
    start_time = time.time()
    correct = 0
    for i in range(X_test.shape[0]):
        pred = n_b.predict(X_test[i])
        if pred == y_test[i]:
            correct += 1
    accuracy = correct / X_test.shape[0]

    end_time = time.time()
    testing_time = end_time - start_time
    print(f"Training and testing time are:{training_time:.4f}and {testing_time:.4f}")
    return accuracy

accuracy = (naive_b(X_train, y_train, X_test, y_test, 784))
print(f"Accuracy of 784 dimension MNIST dataset using NB algo:-{(accuracy*100):.4f} %")
# (naive_b(X_train, y_train, X_test, y_test, 784))

print(f"classification on the test set are\n{y_pred}")

