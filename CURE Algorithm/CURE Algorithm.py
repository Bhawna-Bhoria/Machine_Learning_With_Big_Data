# -*- coding: utf-8 -*-
"""MLBD_Assign1Ques2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PC1EB8zqq4ebkAY49etIiBh8eLl3508X
"""


import numpy as np
import scipy.spatial.distance as distance
import sys
import pandas as pd
from sklearn.preprocessing import StandardScaler
from google.colab import files

from scipy.spatial.distance import cdist
from google.colab import auth
from oauth2client.client import GoogleCredentials
from sklearn.metrics import adjusted_rand_score
# from sklearn.datasets import load_digits
!pip install -U -q PyDrive
from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from sklearn.cluster import KMeans

# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)
train_data= ["/content/train.csv"]
id="1vzOEbtvk-s5h7SNiYbMpmleZ1tIxoheQ"
downloaded = drive.CreateFile({'id':id}) 
downloaded.GetContentFile('train.csv.zip')

initial_data = pd.read_csv('train.csv.zip')
initial_data

initial_df=np.array(initial_data)
initial_df





scaler = StandardScaler().fit(initial_df)
initial_df = scaler.transform(initial_df)


df=pd.DataFrame(initial_df)

# df.to_csv('Preprocess_data.csv', encoding = 'utf-8-sig') 
# files.download('Preprocess_data.csv')
print("Normalised Data")
df

from google.colab import drive
drive.mount('/content/drive')




def calculate_dist(vecA, vecB):
    return np.sqrt(np.power(vecA - vecB, 2).sum())
class CreateCluster:
    def __init__(self, id__, center__):
        self.points = center__
        self.repPoints = center__
        self.center = center__
        self.index = [id__]
        
    def __repr__(self):
        return "Cluster " + " Size: " + str(len(self.points))
    
    def GetCentroids(self, clust):
        totalPoints_1 = len(self.index)
        totalPoints_2 = len(clust.index)
        self.center = (self.center*totalPoints_1 + clust.center*totalPoints_2) / (totalPoints_1 + totalPoints_2)
    
    # Computes and stores representative points for this cluster
    def GetRepPoints(self, numRepPoints, alpha):
        tempSet = None
        for i in range(1, numRepPoints+1):
            maxDist = 0
            maxPoint = None
            for p in range(0, len(self.index)):
                if i == 1:
                    minDist = calculate_dist(self.points[p,:], self.center)
                else:
                    X = np.vstack([tempSet, self.points[p, :]])
                    tmpDist = distance.pcalculate_dist(X)
                    minDist = tmpDist.min()
                if minDist >= maxDist:
                    maxDist = minDist
                    maxPoint = self.points[p,:]
            if tempSet is None:
                tempSet = maxPoint
            else:
                tempSet = np.vstack((tempSet, maxPoint))
        for j in range(len(tempSet)):
            if self.repPoints is None:
                self.repPoints = tempSet[j,:] + alpha * (self.center - tempSet[j,:])
            else:
                self.repPoints = np.vstack((self.repPoints, tempSet[j,:] + alpha * (self.center - tempSet[j,:])))

    def Representative_distance(self, clust):
        Representative_distance = float('inf')
        for repA in self.repPoints:
            if type(clust.repPoints[0]) != list:
                repB = clust.repPoints
                distTemp = calculate_dist(repA, repB)
                if distTemp < Representative_distance:
                    Representative_distance = distTemp
            else:
                for repB in clust.repPoints:
                    distTemp = calculate_dist(repA, repB)
                    if distTemp < Representative_distance:
                        Representative_distance = distTemp
        return Representative_distance

    def ClusterMerging(self, clust, numRepPoints, alpha):
        self.GetCentroids(clust)
        self.points = np.vstack((self.points, clust.points))
        self.index = np.append(self.index, clust.index)
        self.repPoints = None
        self.GetRepPoints(numRepPoints, alpha)


def CallingAlgo(data, numRepPoints, alpha, numDesCluster):

    # Initialization
    Clusters = []
    numCluster = len(data)
    numPts = len(data)
    distCluster = np.ones([len(data), len(data)])
    distCluster = distCluster * float('inf')
    for idPoint in range(len(data)):
        newClust = CreateCluster(idPoint, data[idPoint,:])
        Clusters.append(newClust)
    for row in range(0, numPts):
    	for col in range(0, row):
    		distCluster[row][col] = calculate_dist(Clusters[row].center, Clusters[col].center)
    while numCluster > numDesCluster:
        if np.mod(numCluster, 50) == 0:
            print('Number of Clusters at present :', numCluster)

        # Find a pair of closet clusters
        minIndex = np.where(distCluster == np.min(distCluster))
        minIndex1 = minIndex[0][0]
        minIndex2 = minIndex[1][0]

        # Merge
        Clusters[minIndex1].ClusterMerging(Clusters[minIndex2], numRepPoints, alpha)
        # Update the distCluster matrix
        for i in range(0, minIndex1):
            distCluster[minIndex1, i] = Clusters[minIndex1].Representative_distance(Clusters[i])
        for i in range(minIndex1+1, numCluster):
            distCluster[i, minIndex1] = Clusters[minIndex1].Representative_distance(Clusters[i])
        # Delete the merged cluster and its disCluster vector.
        distCluster = np.delete(distCluster, minIndex2, axis=0)
        distCluster = np.delete(distCluster, minIndex2, axis=1)
        del Clusters[minIndex2]
        numCluster = numCluster - 1

    print('Number of Clusters at last:', numCluster)
    Label = [0] * numPts
    for i in range(0, len(Clusters)):
        for j in range(0, len(Clusters[i].index)):
            Label[Clusters[i].index[j]] = i + 1

    return Label

import math

def calculate_entropy(Label_pre,Label_true):
  df = pd.DataFrame(Label_pre,columns =['pred_labels'])

  # df-=1 

  # df = pd.DataFrame(Label_true,columns =['labels_true'])
  df['labels_true'] = Label_true 
  # df['pred_labels'] = Label_pre
  df['match'] = np.where((df['labels_true'] == df['pred_labels']), 1, 0)
  prob_list=[]
  total_count=data.shape[0]
  total_match=0
  count=0
  # print(df["pred_labels"].unique())
  for i in range(10):
    total_count=len(df[df['labels_true']==i])+1
    print(" ")
    print(f"class {i}")
    print(total_count)
    # print(np.sum(np.where((df['pred_labels'] == i) & (df['match'] == 1), 1, 0)))
    total_match=np.sum(np.where((df['pred_labels'] == i) & (df['match'] == 1), 1, 0))+1
    print(total_match)
    prob_match = total_match/total_count
    print(f"prob_match is {prob_match}")
    entropy=-(prob_match*math.log(prob_match)+(1-prob_match)*math.log(1-prob_match))
    print(f"Calculated entropy for class {i+1} : {entropy}")
    prob_list.append(entropy)
  prob_list = [x / 10 for x in prob_list] # because we have 10 clusters
  # prob_list=prob_list/10 
  total_entropy = np.sum(prob_list)
  print(f"total entropy is {total_entropy}")

  # total_unmatch=np.sum(np.where((df['pred_labels'] == i) & (df['match'] == 0), 1, 0))+1
  # prob_unmatch = total_unmatch/total_count
  # print(prob_unmatch)

  # count=-(count+prob*math.log(prob))
  # print(f"Calculated entropy for class {i+1} : {count}")




  # print(np.where((df['pred_labels'] == i) & (df['match'] == 1), 1, 0))
  # total_match+=np.where((df['pred_labels'] == i) & (df['match'] == 1), 1, 0)
  # print(total_match)
  # prob = total_match/total_count
  # print(f"Probability for cluster {i} is {prob}")

from numpy.core.multiarray import tracemalloc_domain

import sys,time
import numpy as np
import matplotlib.pyplot as plt
from sklearn import metrics

# The number of representative points
numRepPoints = 10
# Shrink factor
alpha = 0.1
# Desired cluster number
numDesCluster = 10

# start = time.clock()
org_data = pd.read_csv('/content/sample_data/mnist_train_small.csv', header = None)
org_data = np.array(org_data)
org_data
x,y= org_data.shape
data_set = org_data[0:1000]
Label_true = data_set.T[0]
data = data_set[:,1:999]
# data_set = np.loadtxt('./3clus.txt')
# data = tracemalloc_domain[:,1:999]
# data = data_set[:,0:2]
# Label_true = data_set[:,2]
# print("Please wait for CURE clustering to accomplete...")
Label_pre = CallingAlgo(data, numRepPoints, alpha, numDesCluster)
print("Clustering completed\n")
# end = time.clock()
# print("The time of CURE algorithm is", end - start, "\n")
# Compute the NMI
nmi = metrics.v_measure_score(Label_true, Label_pre)
# nmi = metrics.adjusted_rand_score(Label_true, Label_pre)
print("Similarity Score =", nmi)
calculate_entropy(Label_pre,Label_true)

data_set = org_data[0:100]
Label_true = data_set.T[0]
data = data_set[:,1:99]
# data_set = np.loadtxt('./3clus.txt')
# data = tracemalloc_domain[:,1:999]
# data = data_set[:,0:2]
# Label_true = data_set[:,2]
# print("Please wait for CURE clustering to accomplete...")
Label_pre = CallingAlgo(data, numRepPoints, alpha, numDesCluster)
print("Clustering completed\n")
# end = time.clock()
# print("The time of CURE algorithm is", end - start, "\n")
# Compute the NMI
nmi = metrics.v_measure_score(Label_true, Label_pre)
# nmi = metrics.adjusted_rand_score(Label_true, Label_pre)
print("Similarity Score =", nmi)
calculate_entropy(Label_pre,Label_true)

data_set = org_data[0:200]
Label_true = data_set.T[0]
data = data_set[:,1:199]
# data_set = np.loadtxt('./3clus.txt')
# data = tracemalloc_domain[:,1:999]
# data = data_set[:,0:2]
# Label_true = data_set[:,2]
# print("Please wait for CURE clustering to accomplete...")
Label_pre = CallingAlgo(data, numRepPoints, alpha, numDesCluster)
print("Clustering completed\n")
# end = time.clock()
# print("The time of CURE algorithm is", end - start, "\n")
# Compute the NMI
nmi = metrics.v_measure_score(Label_true, Label_pre)
# nmi = metrics.adjusted_rand_score(Label_true, Label_pre)
print("Similarity Score =", nmi)
calculate_entropy(Label_pre,Label_true)

data_set = org_data[0:300]
Label_true = data_set.T[0]
data = data_set[:,1:299]
# data_set = np.loadtxt('./3clus.txt')
# data = tracemalloc_domain[:,1:999]
# data = data_set[:,0:2]
# Label_true = data_set[:,2]
# print("Please wait for CURE clustering to accomplete...")
Label_pre = CallingAlgo(data, numRepPoints, alpha, numDesCluster)
print("Clustering completed\n")
# end = time.clock()
# print("The time of CURE algorithm is", end - start, "\n")
# Compute the NMI
nmi = metrics.v_measure_score(Label_true, Label_pre)
# nmi = metrics.adjusted_rand_score(Label_true, Label_pre)
print("Similarity Score =", nmi)
calculate_entropy(Label_pre,Label_true)

data_set = org_data[0:400]
Label_true = data_set.T[0]
data = data_set[:,1:399]
# data_set = np.loadtxt('./3clus.txt')
# data = tracemalloc_domain[:,1:999]
# data = data_set[:,0:2]
# Label_true = data_set[:,2]
# print("Please wait for CURE clustering to accomplete...")
Label_pre = CallingAlgo(data, numRepPoints, alpha, numDesCluster)
print("Clustering completed\n")
# end = time.clock()
# print("The time of CURE algorithm is", end - start, "\n")
# Compute the NMI
nmi = metrics.v_measure_score(Label_true, Label_pre)
# nmi = metrics.adjusted_rand_score(Label_true, Label_pre)
print("Similarity Score =", nmi)
calculate_entropy(Label_pre,Label_true)

data_set = org_data[0:500]
Label_true = data_set.T[0]
data = data_set[:,1:499]
# data_set = np.loadtxt('./3clus.txt')
# data = tracemalloc_domain[:,1:999]
# data = data_set[:,0:2]
# Label_true = data_set[:,2]
# print("Please wait for CURE clustering to accomplete...")
Label_pre = CallingAlgo(data, numRepPoints, alpha, numDesCluster)
print("Clustering completed\n")
# end = time.clock()
# print("The time of CURE algorithm is", end - start, "\n")
# Compute the NMI
nmi = metrics.v_measure_score(Label_true, Label_pre)
# nmi = metrics.adjusted_rand_score(Label_true, Label_pre)
print("Similarity Score =", nmi)
calculate_entropy(Label_pre,Label_true)

